# Generated by Django 5.0.3 on 2024-04-16 18:11

from __future__ import annotations

from enum import Enum
from typing import TYPE_CHECKING

from django.conf import settings
from django.db import migrations
from django.db.backends.base.schema import BaseDatabaseSchemaEditor
from django.db.migrations.state import StateApps
from sentry_kafka_schemas.schema_types.group_attributes_v1 import GroupAttributesSnapshot

from sentry.issues.attributes import produce_snapshot_to_kafka
from sentry.new_migrations.migrations import CheckedMigration
from sentry.utils import redis
from sentry.utils.iterators import chunked
from sentry.utils.query import RangeQuerySetWrapperWithProgressBarApprox

if TYPE_CHECKING:
    from sentry.models.group import Group

CHUNK_SIZE = 10000


class GroupOwnerType(Enum):
    SUSPECT_COMMIT = 0
    OWNERSHIP_RULE = 1
    CODEOWNERS = 2


def _bulk_retrieve_snapshot_values(
    group_ids: list[int], Group: type[Group]
) -> list[GroupAttributesSnapshot]:
    group_values_map = {
        id: priority
        for id, priority in Group.objects.filter(id__in=group_ids).values_list("id", "priority")
    }
    assert len(group_values_map) == len(group_ids)

    snapshots = []
    for id, priority in group_values_map.items():
        snapshot: GroupAttributesSnapshot = {
            "group_id": id,
            "priority": priority,
        }
        snapshots.append(snapshot)

    return snapshots


def bulk_send_snapshot_values(
    group_ids: list[int],
    Group: type[Group],
) -> None:
    snapshots = _bulk_retrieve_snapshot_values(group_ids, Group)
    for snapshot in snapshots:
        produce_snapshot_to_kafka(snapshot)


def backfill_group_attributes_to_snuba(
    apps: StateApps, schema_editor: BaseDatabaseSchemaEditor
) -> None:
    Group = apps.get_model("sentry", "Group")

    backfill_key = "backfill_group_priority_to_snuba_progress"
    redis_client = redis.redis_clusters.get(settings.SENTRY_MONITORS_REDIS_CLUSTER)

    progress_id = int(redis_client.get(backfill_key) or 0)

    for group_ids in chunked(
        RangeQuerySetWrapperWithProgressBarApprox(
            Group.objects.filter(id__gt=progress_id).values_list("id", flat=True),
            step=CHUNK_SIZE,
            result_value_getter=lambda item: item,
        ),
        CHUNK_SIZE,
    ):
        bulk_send_snapshot_values(group_ids, Group)
        # Save progress to redis in case we have to restart
        redis_client.set(backfill_key, group_ids[-1], ex=60 * 60 * 24 * 7)


class Migration(CheckedMigration):
    # This flag is used to mark that a migration shouldn't be automatically run in production. For
    # the most part, this should only be used for operations where it's safe to run the migration
    # after your code has deployed. So this should not be used for most operations that alter the
    # schema of a table.
    # Here are some things that make sense to mark as post deployment:
    # - Large data migrations. Typically we want these to be run manually by ops so that they can
    #   be monitored and not block the deploy for a long period of time while they run.
    # - Adding indexes to large tables. Since this can take a long time, we'd generally prefer to
    #   have ops run this and not block the deploy. Note that while adding an index is a schema
    #   change, it's completely safe to run the operation after the code has deployed.
    is_post_deployment = True

    dependencies = [
        ("sentry", "0697_remove_monitor_owner_actor_id_db"),
    ]

    operations = [
        migrations.RunPython(
            backfill_group_attributes_to_snuba,
            reverse_code=migrations.RunPython.noop,
            hints={"tables": ["sentry_groupedmessage"]},
        )
    ]
